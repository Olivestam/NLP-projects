{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLRi7MrQO3Zr"
      },
      "source": [
        "# L5: Dependency parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Vo6jANO3Zw"
      },
      "source": [
        "Dependency parsing is the task of mapping a sentence to a formal representation of its syntactic structure in the form of a dependency tree, which consists of directed arcs between individual words (tokens). In the lab you will implement a dependency parser based on the arc-standard algorithm and the fixed-window model that you implemented in Lab&nbsp;L4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RihXwQwPO3Zx"
      },
      "source": [
        "## The data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcGvtVOPO3Z0"
      },
      "source": [
        "The data set for this lab is the same as for Lab&nbsp;L4: the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org). The code below defines an iterable-style dataset for parser data in the [CoNLL-U format](https://universaldependencies.org/format.html) that the project uses to distribute its data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AL4Q8HJO3Z1"
      },
      "outputs": [],
      "source": [
        "class Dataset():\n",
        "\n",
        "    ROOT = ('<root>', '<root>', 0)  # Pseudo-root\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "\n",
        "    def __iter__(self):\n",
        "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
        "            tmp = [Dataset.ROOT]\n",
        "            for line in lines:\n",
        "                if not line.startswith('#'):  # Skip lines with comments\n",
        "                    line = line.rstrip()\n",
        "                    if line:\n",
        "                        columns = line.split('\\t')\n",
        "                        if columns[0].isdigit():  # Skip range tokens\n",
        "                            tmp.append((columns[1], columns[3], int(columns[6])))\n",
        "                    else:\n",
        "                        yield tmp\n",
        "                        tmp = [Dataset.ROOT]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THpff729O3Z6"
      },
      "source": [
        "We load the training data and the development data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNZlXX-3O3Z6"
      },
      "outputs": [],
      "source": [
        "train_data = Dataset('en_ewt-ud-train-projectivized.conllu')\n",
        "dev_data = Dataset('en_ewt-ud-dev.conllu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV__XUEnO3Z7"
      },
      "source": [
        "Both data sets consist of **parsed sentences**. A parsed sentence is represented as a list of triples, where the first component of each triple (a string) represents a word, and the second component (also a string) represents the word‚Äôs part-of-speech tag. The third component (an integer) specifies the position of the word‚Äôs syntactic head, i.e., its parent in the dependency tree. Run the following code cell to see an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z6_eqMyO3Z9",
        "outputId": "2482408c-e996-4617-c783-409b06eb929a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<root>', '<root>', 0),\n",
              " ('I', 'PRON', 2),\n",
              " ('like', 'VERB', 0),\n",
              " ('yuor', 'PRON', 4),\n",
              " ('blog', 'NOUN', 2),\n",
              " ('.', 'PUNCT', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "example_sentence = list(train_data)[531]\n",
        "\n",
        "example_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHgg66t4O3Z-"
      },
      "source": [
        "In this example the head of the pronoun *I* is the word at position&nbsp;2 ‚Äì the verb *like*. The dependents of *like* are *I* (position&nbsp;1) and the noun *blog* (position&nbsp;4), as well as the final punctuation mark. Note that each sentence starts with the so-called **pseudo-root** (position&nbsp;0). This pseudo-root is a pseudo-word that is guaranteed to be the root of the dependency tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TBCaCg7O3Z_"
      },
      "source": [
        "## Parser interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoT_6hiSO3aG"
      },
      "source": [
        "Like the tagger in the previous lab, the parser that you will implement in this lab follows a simple interface:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N04ukUzBO3aG"
      },
      "outputs": [],
      "source": [
        "class Parser(object):\n",
        "\n",
        "    def predict(self, words, tags):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdIL8Y18O3aH"
      },
      "source": [
        "The single method of this interface has the following specification:\n",
        "\n",
        "**predict** (*self*, *words*, *tags*)\n",
        "\n",
        "> Returns the list of predicted heads (a list of integers) for a single sentence, specified in terms of its *words* (a list of strings) and their corresponding *tags* (also a list of strings).\n",
        "\n",
        "One trivial implementation of this interface is a parser that attaches each (real) word to its preceding word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8vLMXACO3aI"
      },
      "outputs": [],
      "source": [
        "class TrivialParser(Parser):\n",
        "\n",
        "    def predict(self, words, tags):\n",
        "        return [0] + list(range(len(words)-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am__xbYdO3aJ"
      },
      "source": [
        "## Problem 1: Implement an evaluation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcblZx9zO3aJ"
      },
      "source": [
        "Your first task is to implement a function that computes the **unlabelled attachment score (UAS)** of a parser on gold-standard data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-u1Bmm4O3aL"
      },
      "outputs": [],
      "source": [
        "def uas(parser, gold_data):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in gold_data:\n",
        "      predict = parser.predict([t[0] for t in data], [t[1] for t in data])\n",
        "      gold = [t[2] for t in data]\n",
        "      correct += sum(a == b for a, b in zip(predict[1:len(predict)], gold[1:len(gold)]))\n",
        "      total += len(predict)-1\n",
        "    return correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UX6MM21O3aL"
      },
      "source": [
        "Your implementation should conform to the following specification:\n",
        "\n",
        "**uas** (*parser*, *gold_data*)\n",
        "\n",
        "> Computes the unlabelled attachment score of the specified *parser* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. The unlabelled attachment score is the percentage of all tokens to which the parser assigns the correct head (as per the gold standard). The calculation excludes the pseudo-roots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA8t7nDlO3aM"
      },
      "source": [
        "### ü§û Test your code\n",
        "\n",
        "Test your code by computing the unlabelled attachment score for the trivial parser that attaches every word to its preceding word. The expected score on the development set is 9.76%."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = TrivialParser()\n",
        "\n",
        "uas_score = uas(parser, list(dev_data))\n",
        "print(uas_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhsrSZg1PAv2",
        "outputId": "eef0ded7-0a61-4a6a-a3f2-a8be651de343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09757843254204938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_seijJtO3aN"
      },
      "source": [
        "## Problem 2: Create the vocabularies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OM-Fx3VO3aN"
      },
      "source": [
        "The next cell contains skeleton code for a function `make_vocabs` that constructs the two vocabularies of the parser: one for the words and one for the tags. You should be quite familiar with this task by now. You will be able to re-use your code from lab&nbsp;L4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrjygIvpO3aN"
      },
      "outputs": [],
      "source": [
        "PAD = '<pad>'\n",
        "UNK = '<unk>'\n",
        "\n",
        "def make_vocabs(gold_data):\n",
        "    counter = []\n",
        "    vocab_words = {}\n",
        "    vocab_tags = {}\n",
        "    vocab_words[PAD] = len(vocab_words)\n",
        "    vocab_tags[PAD] = len(vocab_tags)\n",
        "    vocab_words[UNK] = len(vocab_words)\n",
        "    for data in gold_data:\n",
        "      for word in data:\n",
        "        if word[0] not in vocab_words:\n",
        "          vocab_words[word[0]] = len(vocab_words)\n",
        "        if word[1] not in vocab_tags:\n",
        "          vocab_tags[word[1]] = len(vocab_tags)\n",
        "    return vocab_words, vocab_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7e4S21QO3aO"
      },
      "source": [
        "Complete the code according to the following specification:\n",
        "\n",
        "**make_vocabs** (*gold_data*)\n",
        "\n",
        "> Returns a pair of dictionaries mapping the unique words and tags in the gold-standard data *gold_data* (an iterable over parsed sentences) to contiguous ranges of integers starting at zero. The word dictionary contains the pseudowords `PAD` (index&nbsp;0) and `UNK` (index&nbsp;1); the tag dictionary contains `PAD` (index&nbsp;0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOlYa1KPO3aP"
      },
      "source": [
        "### ü§û Test your code\n",
        "\n",
        "Test your implementation by computing the total number of unique words and part-of-speech tags in the training data (including the pseudowords and the part-of-speech tag for the pseudoroot). The expected values are 19,676&nbsp;words and 19&nbsp;tags."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words, vocab_tags = make_vocabs(list(train_data))\n",
        "print(len(vocab_words), len(vocab_tags))"
      ],
      "metadata": {
        "id": "dPx-MKD_XR5v",
        "outputId": "bc91c151-04c4-4539-b347-88609a84b701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19676 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdnpZhNrO3aP"
      },
      "source": [
        "## Problem 3: Implement the arc-standard algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tXGIBqqO3aQ"
      },
      "source": [
        "The parser that you will implement in this lab consists of two parts: a static part that implements the logic of the arc-standard algorithm (presented in Lecture&nbsp;5.2), and a non-static part that contains the learning component ‚Äì the fixed-window model that you implemented in Lab&nbsp;L4. In this problem you will implement the static part; the learning component is covered in Problem&nbsp;5.\n",
        "\n",
        "Recall that, in the arc-standard algorithm, the next move (also called ‚Äòtransition‚Äô) of the parser is predicted based on features extracted from the current parser configuration, with references to the words and part-of-speech tags of the input sentence. On the Python side of things, the words and part-of-speech tags are represented as lists of strings, and a configuration is represented as a triple\n",
        "\n",
        "$$\n",
        "(i, \\mathit{stack}, \\mathit{heads})\n",
        "$$\n",
        "\n",
        "where $i$ is an integer specifying the position of the next word in the buffer, $\\mathit{stack}$ is a list of integers specifying the positions of the words currently on the stack (with the topmost element last in the list), and $\\mathit{heads}$ is a list of integers specifying the positions of the head words. If a word has not yet been assigned a head, its head value is&nbsp;0. To illustrate this representation, the initial configuration for the example sentence above is"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "dYvXzbvbO3aQ"
      },
      "source": [
        "(0, [], [0, 0, 0, 0, 0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4atNWvKpO3aQ"
      },
      "source": [
        "and a possible final configuration is"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "Q2x5iJwQO3aR"
      },
      "source": [
        "(6, [0], [0, 2, 0, 4, 2, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K48XzSxdO3aT"
      },
      "source": [
        "**Note:** In Lecture&nbsp;5.2, both the buffer and the stack were presented as list of words. Here we only represent the *stack* as a list of words. To represent the *buffer*, we simply record the position of the next word that has not been processed yet (the integer $i$). This acknowledges the fact that the buffer (in contrast to the stack) can never grow, but will be processed from left to right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBqpU_WWO3aT"
      },
      "source": [
        "The cell below contains a complete skeleton for the logic of the arc-standard algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjamXDTcO3aT"
      },
      "outputs": [],
      "source": [
        "class ArcStandardParser(Parser):\n",
        "\n",
        "    MOVES = tuple(range(3))\n",
        "\n",
        "    SH, LA, RA = MOVES  # Parser moves are specified as integers.\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def initial_config(num_words):\n",
        "        i = 0\n",
        "        stack = []\n",
        "        heads = [0 for _ in range(num_words)]\n",
        "        return (i, stack, heads)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def valid_moves(config):\n",
        "        valid = [0,1,2]\n",
        "        if len(config[1]) <= 1:\n",
        "          SH = 1\n",
        "          del valid[1]\n",
        "          del valid[1]\n",
        "        if config[0] >= len(config[2]):\n",
        "          del valid[0]\n",
        "\n",
        "        return valid\n",
        "\n",
        "    @staticmethod\n",
        "    def next_config(config, move):\n",
        "        i = config[0]\n",
        "        stack = config[1]\n",
        "        heads = config[2]\n",
        "        if move == 0:\n",
        "          stack.append(i)\n",
        "          i += 1\n",
        "        elif move == 1:\n",
        "          heads[stack[-2]] = stack[-1]\n",
        "          del stack[-2]\n",
        "        elif move == 2:\n",
        "          heads[stack[-1]] = stack[-2]\n",
        "          del stack[-1]\n",
        "        return (i, stack, heads)\n",
        "\n",
        "    @staticmethod\n",
        "    def is_final_config(config):\n",
        "        if len(parser.valid_moves(config)) == 0:\n",
        "          return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WojCFUPVO3aV"
      },
      "source": [
        "Your implementation should conform to the following specification:\n",
        "\n",
        "**initial_config** (*num_words*)\n",
        "\n",
        "> Returns the initial configuration for a sentence with the specified number of words (*num_words*).\n",
        "\n",
        "**valid_moves** (*config*)\n",
        "\n",
        "> Returns the list of valid moves for the specified configuration (*config*).\n",
        "\n",
        "**next_config** (*config*, *move*)\n",
        "\n",
        "> Applies the *move* in the specified configuration *config* and returns the new configuration. This must not modify the input configuration.\n",
        "\n",
        "**is_final_config** (*config*)\n",
        "\n",
        "> Tests whether *config* is a final configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U06rhlqEO3aV"
      },
      "source": [
        "### ü§û Test your code\n",
        "\n",
        "To test your implementation, you can run the code below. The code in this cell creates the initial configuration for the example sentence, simulates a sequence of moves, and then tests that the resulting configuration is the expected final configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElF5RyE0O3aW",
        "outputId": "5d3fcc40-6d51-461b-d1a8-a0aad6b86829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looks good!\n"
          ]
        }
      ],
      "source": [
        "moves = [0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 2]    # 0 = SH, 1 = LA, 2 = RA\n",
        "\n",
        "parser = ArcStandardParser()\n",
        "config = parser.initial_config(len(example_sentence))\n",
        "for move in moves:\n",
        "    assert move in parser.valid_moves(config)\n",
        "    config = parser.next_config(config, move)\n",
        "assert parser.is_final_config(config)\n",
        "assert config == (6, [0], [0, 2, 0, 4, 2, 2])\n",
        "\n",
        "print('Looks good!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W48-Bj30O3aW"
      },
      "source": [
        "## Problem 4: Implement the oracle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GOcWRujO3aX"
      },
      "source": [
        "The learning component of the parser is the next move classifier. To train this classifier, we need training examples of the form $(\\mathbf{x}, m)$, where $\\mathbf{x}$ is a feature vector extracted from a given parser configuration $c$, and $m$ is the corresponding gold-standard move. To obtain $m$, we need an **oracle**.\n",
        "\n",
        "Recall that, in the context of transition-based dependency parsing, an oracle is a function that translates a gold-standard dependency tree (here represented as a list of head ids) into a sequence of moves such that, when the parser takes the moves starting from the initial configuration, then it recreates the original dependency tree. Here we ask you to implement the static oracle that was presented in Lecture&nbsp;5.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RrRnIQ9O3aY"
      },
      "outputs": [],
      "source": [
        "def oracle_moves(gold_heads):\n",
        "\n",
        "    parser = ArcStandardParser()\n",
        "    config = parser.initial_config(len(gold_heads))\n",
        "    moves = []\n",
        "\n",
        "    for i in range(2*len(gold_heads)-1):\n",
        "        valid_moves = parser.valid_moves(config)\n",
        "        if len(valid_moves) == 1:\n",
        "          yield config, valid_moves[0]\n",
        "          config =  parser.next_config(config, valid_moves[0])\n",
        "          moves.append(valid_moves[0])\n",
        "\n",
        "        else:\n",
        "          # choose LA\n",
        "          counterGoldSecond = [1 for i in gold_heads if i ==  config[1][-2]]\n",
        "          counterHeadsSecond = [1 for i in config[2] if i ==  config[1][-2]]\n",
        "\n",
        "          # choose RA\n",
        "          counterGoldTop = [1 for i in gold_heads if i ==  config[1][-1]]\n",
        "          counterHeadsTop = [1 for i in config[2] if i ==  config[1][-1]]\n",
        "\n",
        "          if (len(counterGoldSecond) == len(counterHeadsSecond)) and (gold_heads[config[1][-2]] == config[1][-1]):\n",
        "            yield config, 1\n",
        "            config =  parser.next_config(config, 1)\n",
        "            moves.append(1)\n",
        "\n",
        "          elif (len(counterGoldTop) == len(counterHeadsTop)) and (gold_heads[config[1][-1]] == config[1][-2]):\n",
        "            yield config, 2\n",
        "            config =  parser.next_config(config, 2)\n",
        "            moves.append(2)\n",
        "\n",
        "          else:\n",
        "            yield config, 0\n",
        "            config =  parser.next_config(config, 0)\n",
        "            moves.append(0)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK39zXyfO3aY"
      },
      "source": [
        "Your implementation should conform to the following specification:\n",
        "\n",
        "**oracle_moves** (*gold_heads*)\n",
        "\n",
        "> Translates a gold-standard head assignment for a single sentence (*gold_heads*) into the corresponding stream of oracle moves. More specifically, this yields pairs $(c, m)$ where $m$ is a move (an integer, as specified in the `ArcStandardParser` interface) and $c$ is the parser configuration in which $m$ was taken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIyS24ziO3aY"
      },
      "source": [
        "### ü§û Test your code\n",
        "\n",
        "Test your code by running the cell below. This uses your implementation of *oracle_moves* to extract the oracle move sequence from the example sentence and compares it to the gold-standard move sequence *gold_moves*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSYDAWiMO3aZ"
      },
      "outputs": [],
      "source": [
        "gold_heads = [h for w, t, h in example_sentence]\n",
        "gold_moves = [0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 2]\n",
        "\n",
        "assert list(m for _, m in oracle_moves(gold_heads)) == gold_moves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Q3wghXO3aa"
      },
      "source": [
        "## Problem 5: Fixed-window parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX0TRgysO3ab"
      },
      "source": [
        "Now it is time to put everything together. For the full implementation of the fixed-window parser, you will need the correspondents of the four parts of the fixed-window tagger from Lab&nbsp;L4: an implementation of the fixed-window model; a parser that uses the fixed-window model to make predictions; a function that generates the training examples for the parser; and the training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN6TBhxaO3ac"
      },
      "source": [
        "### Problem 5.1: Implement the fixed-window model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFVykbX2O3ac"
      },
      "source": [
        "The fixed-window model for the parser is the same as the fixed-window model for the tagger in Lab&nbsp;L4. You can simply copy your code from that lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwxtRg5HO3ac"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "class FixedWindowModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        moduleList = nn.ModuleList([])\n",
        "        self.embedding_layers = nn.ModuleList([])\n",
        "        self.linear_input = 0\n",
        "        n_words = 0\n",
        "        n_tags = 0\n",
        "        self.k = 0\n",
        "        index = 1\n",
        "        for specs in embedding_specs:\n",
        "          if index == 0:\n",
        "            embedding = nn.Embedding(specs[1], specs[2])\n",
        "            embedding.weight.data.normal_(0.0,0.01)\n",
        "            index += 1\n",
        "          else:\n",
        "              embedding = nn.Embedding(specs[1], specs[2])\n",
        "              embedding.weight.data.normal_(0.0,0.01)\n",
        "          self.embedding_layers.append(embedding)\n",
        "          self.linear_input += specs[0]*specs[2]\n",
        "          self.k += specs[0]\n",
        "          if specs[1] > n_words:\n",
        "            n_words = specs[1]\n",
        "          if specs[1] != n_words:\n",
        "            n_tags = specs[1]\n",
        "        self.instances = [value[0] for value in embedding_specs]\n",
        "        self.hidden_linear = nn.Linear(self.linear_input, hidden_dim, bias = True)\n",
        "        self.relu = torch.relu\n",
        "        self.linear = nn.Linear(hidden_dim, 3, bias = True)\n",
        "\n",
        "    def forward(self, features):\n",
        "        index = 0\n",
        "        embedd = []\n",
        "        for i in range(len(self.embedding_layers)):\n",
        "          embedd.append(self.embedding_layers[i](features[:,index:index+self.instances[i]].long()))\n",
        "          index += self.instances[i]\n",
        "          if self.instances[i] > 1 :\n",
        "            embedd[i] = embedd[i].view(len(features),1,-1)\n",
        "        concat = torch.cat(tuple(embedd), dim=2)\n",
        "        hidden = self.hidden_linear(concat)\n",
        "        relu = self.relu(hidden)\n",
        "        output = self.linear(relu)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7JUmbt9O3ad"
      },
      "source": [
        "### Problem 5.2: Implement the parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mGPH3l-O3ad"
      },
      "source": [
        "The next step is to implement the parser itself. This parser will use the fixed-window model to predict the next move for a given configuration in the arc-standard algorithm, based on the features extracted from the current feature window.\n",
        "\n",
        "#### Default feature model\n",
        "\n",
        "For the parser, we ask you to implement a fixed-window model with the following features ($k=6$):\n",
        "\n",
        "0. word form of the next word in the buffer\n",
        "1. word form of the topmost word on the stack\n",
        "2. word form of the second-topmost word on the stack\n",
        "3. part-of-speech tag of the next word in the buffer\n",
        "4. part-of-speech tag of the topmost word on the stack\n",
        "5. part-of-speech tag of the second-topmost word on the stack\n",
        "\n",
        "Whenever the value of a feature is undefined, you should use the special value `PAD`.\n",
        "\n",
        "#### Hyperparameters\n",
        "\n",
        "The following choices are reasonable defaults for the hyperparameters of the network architecture used by the parser:\n",
        "\n",
        "* width of the word embedding: 50\n",
        "* width of the tag embedding: 10\n",
        "* size of the hidden layer: 180"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAY-fP18O3ad"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class FixedWindowParser(ArcStandardParser):\n",
        "\n",
        "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=180):\n",
        "        self.embedding_specs = [(3, len(vocab_words), word_dim), (3, len(vocab_tags), tag_dim)]\n",
        "        self.model = FixedWindowModel(self.embedding_specs, len(vocab_tags), hidden_dim)\n",
        "        self.vocab_words = vocab_words\n",
        "        self.vocab_tags = vocab_tags\n",
        "\n",
        "    def featurize(self, words, tags, config):\n",
        "        output = torch.zeros(6)\n",
        "        # Add next word and tag in buffer\n",
        "        if config[0] < len(words):\n",
        "          output[0] = words[config[0]]\n",
        "          output[3] = tags[config[0]]\n",
        "        else:\n",
        "          output[0] = self.vocab_words['<pad>']\n",
        "          output[3] = self.vocab_tags['<pad>']\n",
        "        # Add top most and second top words and their tags from stack\n",
        "        if len(config[1]) > 1:\n",
        "          output[1] = words[config[1][-1]]\n",
        "          output[4] = tags[config[1][-1]]\n",
        "          output[2] = words[config[1][-2]]\n",
        "          output[5] = tags[config[1][-2]]\n",
        "        elif len(config[1]) > 0:\n",
        "          output[1] = words[config[1][-1]]\n",
        "          output[4] = tags[config[1][-1]]\n",
        "          output[2] = self.vocab_words['<pad>']\n",
        "          output[5] = self.vocab_tags['<pad>']\n",
        "        else:\n",
        "          output[1] = self.vocab_words['<pad>']\n",
        "          output[4] = self.vocab_tags['<pad>']\n",
        "          output[2] = self.vocab_words['<pad>']\n",
        "          output[5] = self.vocab_tags['<pad>']\n",
        "        return output\n",
        "\n",
        "    def predict(self, words, tags):\n",
        "        words_id = torch.zeros(len(words))\n",
        "        tags_id = torch.zeros(len(tags))\n",
        "        for i in range(len(words)):\n",
        "          tags_id[i] = self.vocab_tags[tags[i]]\n",
        "          if words[i] in self.vocab_words:\n",
        "            words_id[i] = self.vocab_words[words[i]]\n",
        "          else:\n",
        "            words_id[i] = self.vocab_words['<unk>']\n",
        "        parser = ArcStandardParser()\n",
        "        config = parser.initial_config(len(words))\n",
        "        while not parser.is_final_config(config):\n",
        "          features = self.featurize(words_id, tags_id, config)\n",
        "          features = features.unsqueeze(0)\n",
        "          moves = self.model.forward(features)\n",
        "          moves = moves.squeeze(0)\n",
        "          moves = moves.squeeze(0)\n",
        "          best_move = None\n",
        "          highest_score = - math.inf\n",
        "          valid_moves = parser.valid_moves(config)\n",
        "          for i in range(len(moves)):\n",
        "            if moves[i] > highest_score:\n",
        "              if i in valid_moves:\n",
        "                highest_score = moves[i]\n",
        "                best_move = i\n",
        "          config = parser.next_config(config, best_move)\n",
        "        return config[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgSd3pHpO3af"
      },
      "source": [
        "Complete the skeleton code by implementing the methods of this interface:\n",
        "\n",
        "**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n",
        "\n",
        "> Creates a new fixed-window model of appropriate dimensions and sets up any other data structures that you consider relevant. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n",
        "\n",
        "**featurize** (*self*, *words*, *tags*, *config*)\n",
        "\n",
        "> Extracts features from the specified parser state according to the feature model given above. The state is specified in terms of the words in the input sentence (*words*, a list of word ids), their part-of-speech tags (*tags*, a list of tag ids), and the parser configuration proper (*config*, as specified in Problem&nbsp;3).\n",
        "\n",
        "**predict** (*self*, *words*, *tags*)\n",
        "\n",
        "> Predicts the list of all heads for the input sentence. This simulates the arc-standard algorithm, calling the move classifier whenever it needs to take a decision. The input sentence is specified in terms of the list of its words (strings) and the list of its tags (strings). Both of these should include the pseudoroot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV6-Sy2sO3ag"
      },
      "source": [
        "#### üí° Hint on the implementation\n",
        "\n",
        "In the *predict* function, you must make sure to only execute valid moves. One simple way to do so is to let the fixed-window model predict scores for all moves, and to implement your own, customised argmax operation to find the *valid* move with the highest score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx885YyeO3ag"
      },
      "source": [
        "### Problem 5.3: Generate the training examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZCw9hILO3ah"
      },
      "source": [
        "Your next task is to implement a function that generates the training examples for the parser. You will train as usual, using minibatch training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7HgXunBO3ai"
      },
      "outputs": [],
      "source": [
        "def training_examples(vocab_words, vocab_tags, gold_data, parser, batch_size=100):\n",
        "    batch = torch.zeros((batch_size,6))\n",
        "    moves = torch.zeros(batch_size)\n",
        "    index = 0\n",
        "    for data in gold_data:\n",
        "      gold_heads = [t[2] for t in data]\n",
        "     # print(gold_heads)\n",
        "      words_id = torch.zeros(len(data))\n",
        "      tags_id = torch.zeros(len(data))\n",
        "      for i, word in enumerate(data):\n",
        "        if word[0] in vocab_words:\n",
        "          words_id[i] = vocab_words[word[0]]\n",
        "        else:\n",
        "          words_id[i] = vocab_words['<unk>']\n",
        "        tags_id[i] = vocab_tags[word[1]]\n",
        "      config = parser.initial_config(len(words_id))\n",
        "      for config, move in oracle_moves(gold_heads):\n",
        "        row = parser.featurize(words_id, tags_id, config)\n",
        "        moves[index] = move\n",
        "        batch[index] = row\n",
        "        if index == (batch_size - 1):\n",
        "          yield batch, moves\n",
        "          index = -1\n",
        "          batch = torch.zeros((batch_size,6))\n",
        "          moves = torch.zeros(batch_size)\n",
        "        index += 1\n",
        "\n",
        "    if index < batch_size:\n",
        "      batch = batch[0:index]\n",
        "      moves = moves[0:index]\n",
        "      yield batch, moves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn5Hsh99O3ak"
      },
      "source": [
        "Your code should comply with the following specification:\n",
        "\n",
        "**training_examples** (*vocab_words*, *vocab_tags*, *gold_data*, *tagger*, *batch_size* = 100)\n",
        "\n",
        "> Iterates through the given *gold_data* (an iterable of parsed sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *parser*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmfHJwZVO3al"
      },
      "source": [
        "### Problem 5.4: Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZENTLhTbO3al"
      },
      "source": [
        "The last piece of the puzzle is the training loop. This should be straightforward by now. Complete the skeleton code in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG68peRxO3al"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_fixed_window(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
        "    # TODO: Replace the next line with your own code\n",
        "     # Initialize the model and tagger\n",
        "    parser = FixedWindowParser(vocab_words, vocab_tags)\n",
        "\n",
        "    # Initialize the optimizer. Here we use Adam rather than plain SGD\n",
        "    optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        parser.model.train()\n",
        "        for batch, moves in training_examples(vocab_words, vocab_tags, train_data, parser):\n",
        "            optimizer.zero_grad()\n",
        "            output = parser.model.forward(batch)\n",
        "            output = output.squeeze(1)\n",
        "            moves = moves.long()\n",
        "            loss = F.cross_entropy(output, moves)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return parser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thD916zYO3am"
      },
      "source": [
        "Here is the specification of the training function:\n",
        "\n",
        "**train_fixed_window** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
        "\n",
        "> Trains a fixed-window parser from a set of training data *train_data* (an iterable over parsed sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az1YX14NO3an"
      },
      "source": [
        "The next code cell trains a tagger and evaluates it on the development data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiJ0m8IJO3ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd744de8-1e0e-4d2f-9067-518d4b3c09ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7176\n"
          ]
        }
      ],
      "source": [
        "parser = train_fixed_window(train_data, n_epochs=1)\n",
        "print('{:.4f}'.format(uas(parser, dev_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xN0I9_rO3ap"
      },
      "source": [
        "**‚ö†Ô∏è Your submitted notebook must contain output demonstrating at least 68% UAS on the development set.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrDd7_4GO3ap"
      },
      "source": [
        "## Problem 6: Predicted part-of-speech tags (reflection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cRnoq2WO3aq"
      },
      "source": [
        "The data that you have used in this lab so far contains gold-standard part-of-speech tags, which makes the evaluation of your parser somewhat misleading: In a practical system (including the baseline for the standard project), one does not have access to gold-standard tags; instead one has to first tag the sentences with an automatic part-of-speech tagger.\n",
        "\n",
        "The lab directory contains the following alternative versions of the two data for this lab:\n",
        "\n",
        "* `en_ewt-ud-train-projectivized-retagged.conllu`\n",
        "* `en_ewt-ud-dev-retagged.conllu`\n",
        "\n",
        "In each of them, the gold-standard part-of-speech tags have been replaced by part-of-speech tags automatically predicted by the tagger from Lab&nbsp;L4.\n",
        "\n",
        "Run an experiment to assess the effect that using predicted part-of-speech tags instead of gold-standard tags has on the unlabelled attachment score of your parser. Document your exploration in a short reflection piece (ca. 150&nbsp;words). Respond to the following prompts:\n",
        "\n",
        "* How did you set up your experiment? What results did you get?\n",
        "* Based on what you know about machine learning, did you expect your results? How do you explain them?\n",
        "* What did you learn? How, exactly, did you learn it? Why does this learning matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We changed to the new files and then ran the model on this data. When using gold-standard part-of-speech tags we got an accuracy of 71.76% but when using automatically predicted part-of-speech tags we got an accuracy of 66.37%. This result was expected since the automatically predicted part-of-sspeech tags are not 100% accurate meaning we will misslead the model sometimes. In this lab we learned about oracle and how dependency parsing works. We also got the chance to convert a tagger into a parser, this was pleasent since we managed to make the last lab work proparly it was quite straight-forward to implement a parser since we only had to change small things."
      ],
      "metadata": {
        "id": "UebyyU1WkjxQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQzxM_MkO3aq"
      },
      "source": [
        "**ü•≥ Congratulations on finishing the last lab in this course! ü•≥**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}