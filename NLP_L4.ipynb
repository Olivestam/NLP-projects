{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSrDz4OcClPr"
      },
      "source": [
        "# L4: Part-of-speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_fBR_cdClPt"
      },
      "source": [
        "Part-of-speech tagging is the task of labelling the words (tokens) of a sentence with parts-of-speech such as noun, adjective, and verb. In this lab you will implement the simple, autoregressive fixed-window tagger that was presented in Lecture&nbsp;4.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JTjULySClPu"
      },
      "source": [
        "## The data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osYPx4flClPu"
      },
      "source": [
        "The data set for the lab is the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org), a corpus containing more than 16,000 sentences (254,000&nbsp;tokens) annotated with, among other things, parts-of-speech. The Universal Dependencies Project distributes its data in the [CoNLL-U format](https://universaldependencies.org/format.html), but for this lab we have converted the data into a simpler format: words and their part-of-speech tags are separated by tabs, sentences are separated by empty lines. The code in the next cell defines a container class for data with this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEnpyyGdClPu"
      },
      "outputs": [],
      "source": [
        "class Dataset():\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "\n",
        "    def __iter__(self):\n",
        "        tmp = []\n",
        "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
        "            for line in lines:\n",
        "                line = line.rstrip()\n",
        "                if line:\n",
        "                    tmp.append(tuple(line.split('\\t')))\n",
        "                else:\n",
        "                    yield tmp\n",
        "                    tmp = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8XwBD3fClPv"
      },
      "source": [
        "We load the training data and the development data for this lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MPN8-rLClPw"
      },
      "outputs": [],
      "source": [
        "train_data = Dataset('train.txt')\n",
        "dev_data = Dataset('dev.txt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUDIqiIMClPw"
      },
      "source": [
        "Both data sets consist of **tagged sentences**. On the Python side of things, a tagged sentence is represented as a list of string pairs, where the first component of each pair represents a word token and the second component represents the word’s tag. The possible tags are listed and exemplified in the [Annotation Guidelines](http://universaldependencies.org/u/pos/all.html) of the Universal Dependencies Project. Run the next code cell to see an example of a tagged sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sob4IPG3ClPw",
        "outputId": "e465b2d1-fbaf-41b6-abea-a6fcd5f51d1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('There', 'PRON'),\n",
              " ('has', 'AUX'),\n",
              " ('been', 'VERB'),\n",
              " ('talk', 'NOUN'),\n",
              " ('that', 'SCONJ'),\n",
              " ('the', 'DET'),\n",
              " ('night', 'NOUN'),\n",
              " ('curfew', 'NOUN'),\n",
              " ('might', 'AUX'),\n",
              " ('be', 'AUX'),\n",
              " ('implemented', 'VERB'),\n",
              " ('again', 'ADV'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "list(train_data)[42]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoiLnvc1ClPx"
      },
      "source": [
        "## Tagger interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4l4Ial4ClPx"
      },
      "source": [
        "The tagger that you will implement in this lab follows a simple interface with just one method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkLtOIQ3ClPx"
      },
      "outputs": [],
      "source": [
        "class Tagger(object):\n",
        "\n",
        "    def predict(self, sentence):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLXrlUY-ClPx"
      },
      "source": [
        "The single method of this interface has the following specification:\n",
        "\n",
        "**predict** (*self*, *sentence*)\n",
        "\n",
        "> Returns the list of predicted tags (a list of strings) for a single *sentence* (a list of string tokens).\n",
        "\n",
        "One trivial implementation of this interface is a tagger that always predicts the same tag for every word, independently of the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tasIBB3pClPy"
      },
      "outputs": [],
      "source": [
        "class ConstantTagger(Tagger):\n",
        "\n",
        "    def __init__(self, the_tag):\n",
        "        self.the_tag = the_tag\n",
        "\n",
        "    def predict(self, words):\n",
        "        return [self.the_tag] * len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8aElG8IClPy"
      },
      "source": [
        "## Problem 1: Implement an evaluation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfpBuL0DClPy"
      },
      "source": [
        "Your first task is to implement a function that computes the accuracy of a tagger on gold-standard data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1YbltRBClPy"
      },
      "outputs": [],
      "source": [
        "def accuracy(tagger, gold_data):\n",
        "    # TODO: Replace the next line with your own code\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for gold in gold_data:\n",
        "      words = [j[0] for j in gold]\n",
        "      predict = tagger.predict(words)\n",
        "      for j in range(len(predict)):\n",
        "        if predict[j] == gold[j][1]:\n",
        "          correct +=1\n",
        "        total += 1\n",
        "    return correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpRFyIZAClPy"
      },
      "source": [
        "Your implementation should conform to the following specification:\n",
        "\n",
        "**accuracy** (*tagger*, *gold_data*)\n",
        "\n",
        "> Computes the accuracy of the *tagger* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. Recall that the accuracy is defined as the percentage of tokens to which the tagger assigns the correct tag (as per the gold standard)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_pwGHMyClPz"
      },
      "source": [
        "### 🤞 Test your code\n",
        "\n",
        "Test your code by computing the accuracy on the development set of a trivial tagger that tags each word as a noun. The expected value is 16.69%."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tagger = ConstantTagger('NOUN')\n",
        "\n",
        "accuracy(tagger, dev_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlixc7vyDzaT",
        "outputId": "da7cc03c-0b2d-49bb-f969-45154fd02a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1668919993637665"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPIyZi9JClPz"
      },
      "source": [
        "## Problem 2: Implement a baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnM7Xk3YClPz"
      },
      "source": [
        "Before you start working on the tagger as such, we ask you to first implement a simple baseline:\n",
        "\n",
        "> Tag each input word with the most frequent tag for that word in the training data. If an input word does not occur in the training data, tag it with the overall most frequent tag in the training data. Break ties by choosing that tag which comes first in the alphabetical order.\n",
        "\n",
        "To implement the baseline, you need to implement both a class `BaselineTagger` and a function `train_baseline`. A `BaselineTagger` has two fields: a dictionary mapping each word in the training data to the most frequent tag for that word, and a string representing the fallback tag (overall most frequent tag in the training data). Both of these fields are set in the `train_baseline` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9US5o0DClPz"
      },
      "outputs": [],
      "source": [
        "class BaselineTagger(Tagger):\n",
        "    def __init__(self):\n",
        "        self.most_frequent = {}\n",
        "        self.fallback = None\n",
        "\n",
        "    def predict(self, words):\n",
        "        # TODO: Replace the next line with your own code\n",
        "        predict = []\n",
        "        for word in words:\n",
        "          if word in self.most_frequent:\n",
        "            predict.append(self.most_frequent[word])\n",
        "          else:\n",
        "            predict.append(self.fallback)\n",
        "        return predict\n",
        "\n",
        "def train_baseline(train_data):\n",
        "    # TODO: Replace the next line with your own code\n",
        "    frequency = {}\n",
        "    most_common = {}\n",
        "    for sublist in train_data:\n",
        "      for word in sublist:\n",
        "        if word[0] in frequency:\n",
        "          if word[1] in most_common:\n",
        "            most_common[word[1]] += 1\n",
        "          else:\n",
        "            most_common[word[1]] = 1\n",
        "          if word[1] in frequency[word[0]]:\n",
        "            frequency[word[0]][word[1]] += 1\n",
        "          else:\n",
        "            frequency[word[0]][word[1]] = 1\n",
        "        else:\n",
        "          frequency[word[0]] = {word[1] : 1}\n",
        "    tagger = BaselineTagger()\n",
        "    tagger.fallback = max(most_common, key=most_common.get)\n",
        "    for word in frequency:\n",
        "      # Sort dictionary so that it chooses on alphabetic order if several tags has the same frequency\n",
        "      frequency[word] = dict(sorted(frequency[word].items()))\n",
        "      frequency[word] = max(frequency[word], key=frequency[word].get)\n",
        "    tagger.most_frequent = frequency\n",
        "\n",
        "    return tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G7Qb1oDClP0"
      },
      "source": [
        "### 🤞 Test your code\n",
        "\n",
        "Test your implementation by computing the accuracy of the baseline tagger on the development data. The expected value is 85.61%."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = train_baseline(list(train_data))\n",
        "\n",
        "accuracy(tagger, dev_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRIIgoKKkYJD",
        "outputId": "d7b797c0-c107-422c-aaab-84221f4206b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8560521711468109"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD699bBhClP0"
      },
      "source": [
        "## Problem 3: Create the vocabularies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF8Ad_0nClP0"
      },
      "source": [
        "As in previous labs, you will need an explicit representation of your vocabulary. Here we actually have two vocabularies: one for the words and one for the tags. Both should be represented as dictionaries that map words/tags to a contiguous range of integers, starting at zero.\n",
        "\n",
        "The next cell contains skeleton code for a function `make_vocabs` that constructs the two vocabularies from gold-standard data. The code cell also defines a name for the ‘unknown word’ (`UNK`) and for an additional pseudoword that you will use as a placeholder for undefined values (`PAD`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnNwUtaOClP0"
      },
      "outputs": [],
      "source": [
        "PAD = '<pad>'\n",
        "UNK = '<unk>'\n",
        "\n",
        "def make_vocabs(gold_data):\n",
        "    # TODO: Replace the next line with your own code\n",
        "    counter = []\n",
        "    vocab_words = {}\n",
        "    vocab_tags = {}\n",
        "    for data in gold_data:\n",
        "      for word in data:\n",
        "        if word[0] not in vocab_words:\n",
        "          vocab_words[word[0]] = len(vocab_words)\n",
        "        if word[1] not in vocab_tags:\n",
        "          vocab_tags[word[1]] = len(vocab_tags)\n",
        "    vocab_words['<pad>'] = len(vocab_words)\n",
        "    vocab_tags['<pad>'] = len(vocab_tags)\n",
        "    vocab_words['<unk>'] = len(vocab_words)\n",
        "\n",
        "    #Make reverse vocab_tags\n",
        "    vocab_tags_reverse = {}\n",
        "    for key, value in vocab_tags.items():\n",
        "      vocab_tags_reverse[value] = key\n",
        "\n",
        "\n",
        "    return vocab_words, vocab_tags, vocab_tags_reverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKy8ndohClP0"
      },
      "source": [
        "Complete the code according to the following specification:\n",
        "\n",
        "**make_vocabs** (*gold_data*)\n",
        "\n",
        "> Returns a pair of dictionaries mapping the unique words and tags in the gold-standard data *gold_data* (an iterable over tagged sentences) to contiguous ranges of integers starting at zero. The word dictionary contains the pseudowords `PAD` (index&nbsp;0) and `UNK` (index&nbsp;1); the tag dictionary contains `PAD` (index&nbsp;0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USEe1_53ClP0"
      },
      "source": [
        "### 🤞 Test your code\n",
        "\n",
        "Test your implementation by computing the total number of unique words and tags in the training data (including the pseudowords). The expected values are 19,674&nbsp;words and 18&nbsp;tags."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words, vocab_tags, vocab_tags_reverse = make_vocabs(train_data)\n",
        "print(len(vocab_words), len(vocab_tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsJ2o4J7LFCa",
        "outputId": "f409971e-4724-47c4-87db-49c09436957c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19674 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYOUlOv0ClP1"
      },
      "source": [
        "## Problem 4: Fixed-window tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfog2-DwClP1"
      },
      "source": [
        "Your main task in this lab is to implement a complete, autoregressive part-of-speech tagger based on the fixed-window architecture. This implementation has four parts: the fixed-window model; a tagger that uses the fixed-window model to make predictions; a function that generates training examples for the tagger; and the training function.\n",
        "\n",
        "**⚠️ We expect that solving this problem will take you the longest time in this lab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3iRHrzZClP1"
      },
      "source": [
        "### Problem 4.1: Implement the fixed-window model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e1fPDv3ClP1"
      },
      "source": [
        "The architecture of the fixed-window model is presented in Lecture&nbsp;4.2. An input to the network takes the form of a $k$-dimensional vector of word ids and/or tag ids. Each integer $i$ is mapped to an $e_i$-dimensional embedding vector. These vectors are concatenated to form a vector of length $e_1 + \\cdots + e_k$, and sent through a feed-forward network with a single hidden layer and a rectified linear unit (ReLU).\n",
        "\n",
        "#### Default features\n",
        "\n",
        "We ask you to implement a fixed-window model with the following features ($k=4$):\n",
        "\n",
        "0. current word\n",
        "1. previous word\n",
        "2. next word\n",
        "3. tag predicted for the previous word\n",
        "\n",
        "Whenever the value of a feature is undefined, you should use the special value `PAD`.\n",
        "\n",
        "#### Embedding specifications\n",
        "\n",
        "To make your implementation of the fixed-window model useful for a range of different applications (including the parser that you will build in lab&nbsp;5), it should support other feature sets than the default model. To this end, the constructor of your model should accept a list of what we call *embedding specifications*. An embedding specification is a triple $(m, n, e)$ consisting of three integers. Such a triple specifies that the model should include $m$ instances of an embedding from $n$ items to vectors of size $e$. All of the $m$ instances are to share their weights. In this lab, the embeddings will be embeddings for words and tags. For example, to instantiate the default feature model, you would initialise the model with the following specifications:\n",
        "\n",
        "``\n",
        "[(3, num_words, word_dim), (1, num_tags, tag_dim)]\n",
        "``\n",
        "\n",
        "This specifies that the model should use 3 instances of an embedding from *num_words* words to vectors of length *word_dim*, and 1 instance of an embedding from *num_tags* tags to vectors of length *tag_dim*. All 3 instances of the word embedding would share their weights. If you rather wanted to have word embeddings with separate weights, you would initialise the model with the following specifications:\n",
        "\n",
        "``\n",
        "[(1, num_words, word_dim), (1, num_words, word_dim), (1, num_words, word_dim), (1, num_tags, tag_dim)]\n",
        "``\n",
        "\n",
        "We recommend that you initialize the weights of each embedding with values drawn from $\\mathcal{N}(0, 10^{-2})$.\n",
        "\n",
        "#### Hyperparameters\n",
        "\n",
        "The network architecture introduces a number of hyperparameters. The following choices are reasonable defaults:\n",
        "\n",
        "* width of each word embedding: 50\n",
        "* width of each tag embedding: 10\n",
        "* size of the hidden layer: 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBuTkv5VClP1"
      },
      "source": [
        "The next cell contains skeleton code for the implementation of the fixed-window model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXZM7zZrClP1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class FixedWindowModel(nn.Module):\n",
        "    def __init__(self, embedding_specs, hidden_dim = 100, output_dim=100):\n",
        "        super().__init__()\n",
        "        moduleList = nn.ModuleList([])\n",
        "        self.embedding_layers = nn.ModuleList([])\n",
        "        self.linear_input = 0\n",
        "        n_words = 0\n",
        "        n_tags = 0\n",
        "        self.k = 0\n",
        "        index = 1\n",
        "        for specs in embedding_specs:\n",
        "          if index == 0:\n",
        "            embedding = nn.Embedding(specs[1], specs[2])\n",
        "            embedding.weight = nn.parameter.Parameter(glove)\n",
        "            index += 1\n",
        "          else:\n",
        "              embedding = nn.Embedding(specs[1], specs[2])\n",
        "              embedding.weight.data.normal_(0.0,0.01)\n",
        "          self.embedding_layers.append(embedding)\n",
        "          self.linear_input += specs[0]*specs[2]\n",
        "          self.k += specs[0]\n",
        "          if specs[1] > n_words:\n",
        "            n_words = specs[1]\n",
        "          if specs[1] != n_words:\n",
        "            n_tags = specs[1]\n",
        "        self.instances = [value[0] for value in embedding_specs]\n",
        "        self.hidden_linear = nn.Linear(self.linear_input, hidden_dim, bias = True)\n",
        "        self.relu = torch.relu\n",
        "        self.linear = nn.Linear(hidden_dim, n_tags, bias = True)\n",
        "\n",
        "    def forward(self, features):\n",
        "        index = 0\n",
        "        embedd = []\n",
        "        for i in range(len(self.embedding_layers)):\n",
        "          embedd.append(self.embedding_layers[i](features[:,index:index+self.instances[i]].long()))\n",
        "          index += self.instances[i]\n",
        "          if self.instances[i] > 1 :\n",
        "            embedd[i] = embedd[i].view(len(features),1,-1)\n",
        "        concat = torch.cat(tuple(embedd), dim=2)\n",
        "        hidden = self.hidden_linear(concat)\n",
        "        relu = self.relu(hidden)\n",
        "        output = self.linear(relu)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onuQ1P05ClP2"
      },
      "source": [
        "Your implementation should meet the following specification:\n",
        "\n",
        "**__init__** (*self*, *embedding_specs*, *hidden_dim*, *output_dim*)\n",
        "\n",
        "> A fixed-window model is initialized with a list of specifications for the embeddings the network should use (*embedding_specs*), the size of the hidden layer (*hidden_dim*), and the size of the output layer (*output_dim*).\n",
        "\n",
        "**forward** (*self*, *features*)\n",
        "\n",
        "> Computes the network output for a given feature representation *features*. This is a tensor of shape $B \\times k$ where $B$ is the batch size (number of samples in the batch) and $k$ is the total number of embeddings specified upon initialisation. For example, for the default feature model, $k=4$, as this model includes 3 (weight-sharing) word embeddings and 1 tag embedding.\n",
        "\n",
        "#### 💡 Hint on the implementation\n",
        "\n",
        "You will have to construct embeddings based on the embedding specifications. It is natural to store these embeddings in a list- or dictionary-valued attribute of the `FixedWindowModel` object. However, in order to expose the embeddings to the auto-differentiation magic of PyTorch (so that their weights are updated during training), you must instead store them in an [`nn.ModuleList`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleList) or [`nn.ModuleDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_specs = [(3, len(vocab_words), 50), (1, len(vocab_tags), 10)]\n",
        "model = FixedWindowModel(embedding_specs)"
      ],
      "metadata": {
        "id": "lM4sBk8JXhee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf8yVbniClP2"
      },
      "source": [
        "### Problem 4.2: Implement the tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoIfr50sClP2"
      },
      "source": [
        "The next step is to implement the tagger itself. The tagger will use the simple algorithm that was presented in Lecture&nbsp;4.2: It processes an input sentence from left to right, and at each position, predicts the tag for the current word based on the features extracted from the current feature window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwGkDftPClP2"
      },
      "outputs": [],
      "source": [
        "class FixedWindowTagger(Tagger):\n",
        "\n",
        "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=100):\n",
        "        self.embedding_specs = [(3, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n",
        "        self.model = FixedWindowModel(embedding_specs, len(vocab_tags), hidden_dim)\n",
        "        self.vocab_words = vocab_words\n",
        "        self.vocab_tags = vocab_tags\n",
        "\n",
        "    def featurize(self, words, i, pred_tags):\n",
        "        output = torch.zeros(4)\n",
        "        output[0] = words[i]\n",
        "        if i == 0:\n",
        "          output[1] = self.vocab_words['<pad>']\n",
        "          output[3] = self.vocab_tags['<pad>']\n",
        "          if len(words)<2:\n",
        "            output[2] = self.vocab_words['<pad>']\n",
        "          else:\n",
        "            output[2] = words[i+1]\n",
        "        elif len(words) == (i+1):\n",
        "          output[1] = words[i-1]\n",
        "          output[3] = pred_tags[i-1]\n",
        "          output[2] = self.vocab_words['<pad>']\n",
        "        else:\n",
        "          output[1] = words[i-1]\n",
        "          output[3] = pred_tags[i-1]\n",
        "          output[2] = words[i+1]\n",
        "        return output\n",
        "\n",
        "    def predict(self, words):\n",
        "        pred_tags = torch.zeros(len(words))\n",
        "        words_id = torch.zeros(len(words))\n",
        "        for i in range(len(words)):\n",
        "          if words[i] in self.vocab_words:\n",
        "            words_id[i] = self.vocab_words[words[i]]\n",
        "          else:\n",
        "            words_id[i] = self.vocab_words['<unk>']\n",
        "        for i in range(len(words_id)):\n",
        "          features = self.featurize(words_id, i, pred_tags)\n",
        "          features = features.unsqueeze(0)\n",
        "          tag = self.model.forward(features)\n",
        "          index_tag = tag.argmax()\n",
        "          pred_tags[i] = index_tag.item()\n",
        "        pred_tags_list = []\n",
        "        for i in range(len(pred_tags)):\n",
        "          pred_tags_list.append(vocab_tags_reverse[int(pred_tags[i].item())])\n",
        "\n",
        "        return pred_tags_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gBRPHrClP2"
      },
      "source": [
        "Complete the skeleton code by implementing the methods of this interface:\n",
        "\n",
        "**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n",
        "\n",
        "> Creates a new fixed-window model of appropriate dimensions and sets up any other data structures that you consider relevant. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n",
        "\n",
        "**featurize** (*self*, *words*, *i*, *pred_tags*)\n",
        "\n",
        "> Extracts features from the specified tagger configuration according to the default feature model. The configuration is specified in terms of the words in the input sentence (*words*, a list of word ids), the position of the current word (*i*), and the list of already predicted tags (*pred_tags*, a list of tag ids). Returns a tensor that can be fed to the fixed-window model.\n",
        "\n",
        "**predict** (*self*, *words*)\n",
        "\n",
        "> Processes the input sentence *words* (a list of string tokens) and makes calls to the fixed-window model to predict the tag of each word. Returns the list of the predicted tags (strings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg0lBaxRClP3"
      },
      "source": [
        "### Problem 4.3: Generate the training examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enw-nYb0ClP3"
      },
      "source": [
        "Your next task is to implement a function that generates the training examples for the tagger. You will train the tagger as usual, using minibatch training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqjevQG1ClP3"
      },
      "outputs": [],
      "source": [
        "def training_examples(vocab_words, vocab_tags, gold_data, tagger, batch_size=100):\n",
        "    batch = torch.zeros((batch_size,4))\n",
        "    tags = torch.zeros(batch_size)\n",
        "    index = 0\n",
        "    for data in gold_data:\n",
        "      words_id = torch.zeros(len(data))\n",
        "      tags_id = torch.zeros(len(data))\n",
        "      for i, word in enumerate(data):\n",
        "        words_id[i] = vocab_words[word[0]]\n",
        "        tags_id[i] = vocab_tags[word[1]]\n",
        "      for i in range(len(words_id)):\n",
        "        row = tagger.featurize(words_id, i, tags_id)\n",
        "        tags[index] = tags_id[i]\n",
        "        batch[index] = row\n",
        "        if index == (batch_size - 1):\n",
        "          yield batch, tags\n",
        "          index = -1\n",
        "          batch = torch.zeros((batch_size,4))\n",
        "          tags = torch.zeros(batch_size)\n",
        "        index += 1\n",
        "    if index < batch_size:\n",
        "      batch = batch[0:index]\n",
        "      tags = tags[0:index]\n",
        "      yield batch, tags\n",
        "\n",
        "tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
        "for batch, tags in training_examples(vocab_words, vocab_tags, train_data, tagger):\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thm3ANFjClP3"
      },
      "source": [
        "Your code should comply with the following specification:\n",
        "\n",
        "**training_examples** (*vocab_words*, *vocab_tags*, *gold_data*, *tagger*, *batch_size* = 100)\n",
        "\n",
        "> Iterates through the given *gold_data* (an iterable of tagged sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *tagger*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-JjDJDuClP3"
      },
      "source": [
        "### Problem 4.4: Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtQUPccXClP3"
      },
      "source": [
        "What remains to be done is the implementation of the training loop. This should be a straightforward generalization of the training loops that you have seen so far. Complete the skeleton code in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R-0StxiClP3"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_fixed_window(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
        "    # Initialize the model and tagger\n",
        "    tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
        "\n",
        "    # Initialize the optimizer. Here we use Adam rather than plain SGD\n",
        "    optimizer = optim.Adam(tagger.model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        tagger.model.train()\n",
        "        for batch, tags in training_examples(vocab_words, vocab_tags, train_data, tagger):\n",
        "            optimizer.zero_grad()\n",
        "            output = tagger.model.forward(batch)\n",
        "            output = output.squeeze(1)\n",
        "            tags = tags.long()\n",
        "            loss = F.cross_entropy(output, tags)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELqN-15FClP4"
      },
      "source": [
        "Here is the specification of the training function:\n",
        "\n",
        "**train_fixed_window** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
        "\n",
        "> Trains a fixed-window tagger from a set of training data *train_data* (an iterable over tagged sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krhEonPUClP4"
      },
      "source": [
        "The next code cell trains a tagger and evaluates it on the development data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q61dpPpnClP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e88c821-d7f4-4000-cd4b-a0a8460f5eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8823\n"
          ]
        }
      ],
      "source": [
        "tagger = train_fixed_window(train_data, n_epochs=2)\n",
        "print('{:.4f}'.format(accuracy(tagger, dev_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRRIAwvuClP4"
      },
      "outputs": [],
      "source": [
        "class Dataset():\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "\n",
        "    def __iter__(self):\n",
        "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
        "            tmp = []\n",
        "            for line in lines:\n",
        "                if not line.startswith('#'):  # Skip lines with comments\n",
        "                    line = line.rstrip()\n",
        "                    if line:\n",
        "                        columns = line.split('\\t')\n",
        "                        if columns[0].isdigit():  # Skip range tokens\n",
        "                            tmp.append(columns)\n",
        "                    else:\n",
        "                        yield tmp\n",
        "                        tmp = []\n",
        "\n",
        "with open('en_ewt-ud-dev-retagged.conllu', 'wt') as target:\n",
        "    for sentence in Dataset('en_ewt-ud-dev.conllu'):\n",
        "        words = [columns[1] for columns in sentence]\n",
        "        for i, t in enumerate(tagger.predict(words)):\n",
        "            sentence[i][3] = t\n",
        "        for columns in sentence:\n",
        "            print('\\t'.join(c for c in columns), file=target)\n",
        "        print(file=target)\n",
        "\n",
        "# (columns[1], columns[3], int(columns[6])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove = torch.load('glove.pt')\n",
        "print(glove.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra8dI4QitYhi",
        "outputId": "82d8a0cc-5656-4fe4-ea56-1eb10ebcb5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19674, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ2ckmQ6ClP4"
      },
      "source": [
        "**⚠️ Your submitted notebook must contain output demonstrating at least 88% accuracy on the development set.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZSJCPzSClP4"
      },
      "source": [
        "## Problem 5: Pre-trained embeddings (reflection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpfgIa5xClP5"
      },
      "source": [
        "Many neural systems for natural language processing use pre-trained word embeddings, either to augment or to replace randomly initialised task-based embeddings. In this problem, you will investigate whether pre-trained embeddings help your part-of-speech tagger.\n",
        "\n",
        "The file `glove.pt` contains a PyTorch tensor containing 50-dimensional pre-trained word embeddings from the [GloVe project](https://nlp.stanford.edu/projects/glove/). You can load this tensor using the command\n",
        "\n",
        "```\n",
        "glove = torch.load('glove.pt')\n",
        "```\n",
        "\n",
        "and should be able to use it as a drop-in replacement for your randomly initialized word embeddings, assuming that the words in your vocabulary are numbered in the order in which they are found in the training data. Have a look at the documentation of the class [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) to learn how to do this.\n",
        "\n",
        "Run experiments to assess the effect that pre-trained embeddings have on (a)&nbsp;the accuracy of the tagger, and (b)&nbsp;the speed of learning, i.e., the number of training examples it takes to reach a certain loss. Document your exploration in a short reflection piece (ca. 150&nbsp;words). Respond to the following prompts:\n",
        "\n",
        "* How did you integrate the pre-trained embeddings into your system? What did you measure? What results did you get?\n",
        "* Based on what you know about word embeddings and transfer learning, did you expect your results? How do you explain them?\n",
        "* What did you learn? How, exactly, did you learn it? Why does this learning matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTUbsLRXClP5"
      },
      "source": [
        "When we intialized our embeddings we used the glove instance instead of those from embedding_specs. We used the glove as a parameter for the first embedding but not on the embedding of the tags since that embedding had different shape. We meassured the speed of learning and the accuracy. From this we could see that the accuracy was pretty much the same using pretrained embeddings as for not pretrained embeddings, for two epochs we got an accuracy of 89 % when not using pretrained embeddings and a accuracy of 89 % when using pretrained embeddings. For the speed of learning the pretrained embeddings went from a loss of around 3 to a loss of around 0.08 over two epochs and this was the same for the not pretrained embeddings. We did expect that pretrained embeddings would be better from the start having a lower loss rate than it had, but that it would learn slower than the not pretrained embeddings. In this lab we learned how to use different embeddings when embedding several inputs using nn.ModuleList()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS9LfodeClP5"
      },
      "source": [
        "**🥳 Congratulations on finishing this lab! 🥳**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "latex_envs": {
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 0
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}